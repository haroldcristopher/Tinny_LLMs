#vLLM

vLLM is a fast and easy-to-use library for LLM inference and serving. 
The project is located at https://github.com/vllm-project/vllm.

vLLM offers an OpenAI-compatible API server option.



