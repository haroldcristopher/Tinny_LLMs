[Unit]
Description=TinyLLM
After=network.target

[Service]
ExecStart=/usr/bin/python3 -m llama_cpp.server --model mistral-7b-instruct-v0.1.Q5_K_M.gguf --host localhost --interrupt_requests False --n_gpu_layers 99 --n_ctx 2048 --chat_format llama-2
WorkingDirectory=/home/ai/tinyllm/llmserver/models
Restart=always
User=ai

[Install]
WantedBy=multi-user.target
